{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#**Add python version you wish** to list\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.6\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1\n",
        "\n",
        "# Choose one of the given alternatives:\n",
        "!sudo update-alternatives --config python3\n",
        "\n",
        "# This one used to work but now NOT(for me)!\n",
        "# !sudo update-alternatives --config python\n",
        "\n",
        "# Check the result\n",
        "!python3 --version\n",
        "\n",
        "# Attention: Install pip (... needed!)\n",
        "!sudo apt install python3-pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YZogbVokNL3",
        "outputId": "0380a0a4-a911-4c5e-a70d-62f0c9616ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.9   2         auto mode\n",
            "  1            /usr/bin/python3.6   1         manual mode\n",
            "  2            /usr/bin/python3.8   1         manual mode\n",
            "  3            /usr/bin/python3.9   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/bin/python3.6 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Python 3.6.15\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 2s (1,071 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122960 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOEsgRUHkuJb",
        "outputId": "633580e0-4e95-4b15-de85-efbf0e9c0936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3-pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retsQZjmkwtp",
        "outputId": "4233587b-5617-4a85-f83a-644ce1b14b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pip is already the newest version (20.0.2-5ubuntu1.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3.6-distutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-WqRGeTk8lH",
        "outputId": "4c2fb2c1-f764-49a3-9eb0-d69a5eb1efa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.6-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.6-distutils python3.6-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 308 kB of archives.\n",
            "After this operation, 1,232 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-lib2to3 all 3.6.15-1+focal3 [122 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-distutils all 3.6.15-1+focal3 [187 kB]\n",
            "Fetched 308 kB in 2s (172 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3.6-lib2to3.\n",
            "(Reading database ... 123324 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.6-lib2to3_3.6.15-1+focal3_all.deb ...\n",
            "Unpacking python3.6-lib2to3 (3.6.15-1+focal3) ...\n",
            "Selecting previously unselected package python3.6-distutils.\n",
            "Preparing to unpack .../python3.6-distutils_3.6.15-1+focal3_all.deb ...\n",
            "Unpacking python3.6-distutils (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-lib2to3 (3.6.15-1+focal3) ...\n",
            "Setting up python3.6-distutils (3.6.15-1+focal3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.1 torchtext==0.6.0"
      ],
      "metadata": {
        "id": "dNOWE_pq6lKc",
        "outputId": "f6cd9691-2e12-4e9c-a407-2d857fe27fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 8.7 kB/s \n",
            "\u001b[?25hCollecting torchtext==0.6.0\n",
            "  Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "Collecting dataclasses; python_version < \"3.7\"\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 66.3 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 230 kB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 61.0 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting importlib-resources; python_version < \"3.7\"\n",
            "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 72.3 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 115 kB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\"\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Installing collected packages: dataclasses, typing-extensions, numpy, torch, zipp, importlib-resources, tqdm, urllib3, charset-normalizer, idna, certifi, requests, sentencepiece, six, torchtext\n",
            "Successfully installed certifi-2022.12.7 charset-normalizer-2.0.12 dataclasses-0.8 idna-3.4 importlib-resources-5.4.0 numpy-1.19.5 requests-2.27.1 sentencepiece-0.1.98 six-1.16.0 torch-1.8.1 torchtext-0.6.0 tqdm-4.64.1 typing-extensions-4.1.1 urllib3-1.26.15 zipp-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "requests",
                  "six",
                  "torch",
                  "torchtext",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IxWDPMSC8yBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff71bd7-996d-48f4-a518-4d9e2f4ec0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py\", line 19, in <module>\n",
            "    from distutils.util import strtobool\n",
            "ModuleNotFoundError: No module named 'distutils.util'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content')"
      ],
      "metadata": {
        "id": "nahCd2X99Ext"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "from torchtext.datasets import IMDB\n",
        "# from torchtext.data import Field, LabelField, BucketIterator\n",
        "from torchtext.data import Field, LabelField, BucketIterator\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# from collators import MLMCollator, SequentialMLMCollator, PermutationMLMCollator, \\\n",
        "#                       TextInfillingCollator, TokenDeletionCollator, DocumentRotationCollator, \\\n",
        "#                       PermutationCollator"
      ],
      "metadata": {
        "id": "KF_VGz2X6JS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7f06bae0-3dc5-4871-e2c8-6971f9cfaef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e9abcaa6f3f5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from torchtext.data import Field, LabelField, BucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/usr/local/lib/python3.9/dist-packages/torchtext/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiTaskCollator(MLMCollator):\n",
        "    def __init__(self, tokenizer, mask_probability, nomask_id=-100, policy=None, special_token_ids=None, masking_schedule=None, nsp_probability=0.5):\n",
        "        super(MultiTaskCollator, self).__init__(tokenizer, mask_probability, nomask_id, policy, special_token_ids, masking_schedule)\n",
        "        self.nsp_probability = nsp_probability\n",
        "\n",
        "    def __call__(self, examples, training_step=None):\n",
        "        input_ids, labels, nsp_labels = self._preprocess_batch(examples)\n",
        "        input_ids, mlm_labels = self.mask_tokens(input_ids=input_ids, training_step=training_step)\n",
        "        return input_ids, mlm_labels, nsp_labels\n",
        "\n",
        "    def _preprocess_batch(self, examples):\n",
        "        # Your existing preprocessing logic for MLM\n",
        "        # ...\n",
        "\n",
        "        # Next Sentence Prediction preprocessing\n",
        "        nsp_labels = []\n",
        "        for i, example in enumerate(examples):\n",
        "            if torch.rand(1 < self.nsp_probability:\n",
        "                # Swap current example with the next one\n",
        "                next_example = examples[(i + 1) % len(examples)]\n",
        "                example = torch.cat((example[:self.tokenizer.sep_token_id + 1], next_example[self.tokenizer.sep_token_id + 1:]))\n",
        "                nsp_labels.append(1)\n",
        "            else:\n",
        "                nsp_labels.append(0)\n",
        "        nsp_labels = torch.tensor(nsp_labels, dtype=torch.long)\n",
        "\n",
        "        return examples, nsp_labels\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, base_model, vocab_size):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.mlm_head = nn.Linear(base_model.config.hidden_size, vocab_size)\n",
        "        self.nsp_head = nn.Linear(base_model.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.base_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        mlm_preds = self.mlm_head(sequence_output)\n",
        "        nsp_preds = self.nsp_head(pooled_output)\n",
        "        return mlm_preds, nsp_preds\n"
      ],
      "metadata": {
        "id": "XlFbilt3WjEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "class DataCollator(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        pass\n",
        "\n",
        "    def _preprocess_batch(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses tensors: returns the current examples in case all the tensors are the same\n",
        "        length, otherwise pads tensors with [PAD] until they are the same length. This function\n",
        "        is made adaptable to both work with (i) tuple of tensors as is the case when __call__() is\n",
        "        used as the collate_fn() function of a torch.utils.data.DataLoader() and with (ii) tensor\n",
        "        with shape (B, ...) where B = batch size (i.e. pre-tensorized)\n",
        "        \"\"\"\n",
        "        if all(x.size(0) == examples[0].size(0) for x in examples):\n",
        "            if isinstance(examples, tuple):\n",
        "                return torch.stack(examples, dim=0)\n",
        "            elif isinstance(examples, torch.Tensor):\n",
        "                return examples\n",
        "            else:\n",
        "                raise ValueError('The type of examples \"%s\" is not recognized!' % str(type(examples)))\n",
        "        else:\n",
        "            if self.tokenizer._pad_token is None:\n",
        "                raise ValueError('Pad token not found in @tokenizer!')\n",
        "            elif isinstance(examples, tuple):\n",
        "                return pad_sequence(examples, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "            elif isinstance(examples, torch.Tensor):\n",
        "                return pad_sequence([example for example in examples], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "            else:\n",
        "                raise ValueError('The type of examples \"%s\" is not recognized!' % str(type(examples)))\n",
        "\n",
        "\n",
        "\n",
        "# from transformers import DataCollator\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "class MLMCollator(DataCollator):\n",
        "    def __init__(self, tokenizer, mask_probability, nomask_id=-100, policy=None, special_token_ids=None, masking_schedule=None):\n",
        "        super(MLMCollator).__init__()\n",
        "        if tokenizer.mask_token is None:\n",
        "            raise ValueError(' Tokenizer is missing the mask token (e.g. [MASK], <mask>, etc.)')\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mask_probability = mask_probability\n",
        "        self.mask_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "        self.nomask_id = nomask_id\n",
        "        self.policy = policy or [0.8, 0.1, 0.1]\n",
        "        if sum(self.policy) != 1.0:\n",
        "            raise ValueError('The elements of @policy should sum up to 1.')\n",
        "        self.special_token_ids = special_token_ids or [self.tokenizer.bos_token_id,\n",
        "                                                        self.tokenizer.eos_token_id,\n",
        "                                                        self.tokenizer.sep_token_id,\n",
        "                                                        self.tokenizer.pad_token_id,\n",
        "                                                        self.tokenizer.cls_token_id]\n",
        "        self.masking_schedule = masking_schedule or []\n",
        "\n",
        "    def update_policy(self, training_step):\n",
        "        for step, new_policy in self.masking_schedule:\n",
        "            if training_step >= step:\n",
        "                self.policy = new_policy\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    def __call__(self, examples, training_step=None):\n",
        "        examples_ = examples.clone()\n",
        "        batch = self._preprocess_batch(examples_)\n",
        "        input_ids, labels = self.mask_tokens(input_ids=batch, training_step=training_step)\n",
        "        return input_ids, labels\n",
        "\n",
        "    def mask_tokens(self, input_ids, training_step=None):\n",
        "        # Update the masking policy based on the current training step\n",
        "        if training_step is not None:\n",
        "            self.update_policy(training_step)\n",
        "            # print('yedyedy')\n",
        "        # ... rest of the mask_tokens method remains unchanged ...\n",
        "\n",
        "        # Preparing masked tokens input and label pairs for MLM\n",
        "        labels = input_ids.clone()\n",
        "        probabilities = torch.full(size=labels.shape, fill_value=self.mask_probability)\n",
        "        special_tokens_mask = []\n",
        "\n",
        "        for labels_ in labels.tolist():\n",
        "            mask = self.tokenizer.get_special_tokens_mask(labels_, already_has_special_tokens=True)\n",
        "            special_tokens_mask.append(mask)\n",
        "\n",
        "        special_indices = torch.tensor(data=special_tokens_mask, dtype=torch.bool)\n",
        "        probabilities.masked_fill_(special_indices, value=0.0)\n",
        "\n",
        "        for special_token_id in self.special_token_ids:\n",
        "            if special_token_id:\n",
        "                special_indices = labels.eq(special_token_id)\n",
        "                probabilities.masked_fill_(mask=special_indices, value=0.0)\n",
        "\n",
        "        masked_indices = torch.bernoulli(probabilities).bool()\n",
        "        labels[~masked_indices] = self.nomask_id\n",
        "\n",
        "        replaced_indices = torch.bernoulli(torch.full(size=labels.shape, fill_value=self.policy[0])).bool()\n",
        "        replaced_indices = replaced_indices & masked_indices\n",
        "        input_ids[replaced_indices] = self.mask_id\n",
        "\n",
        "        randomized_indices = torch.bernoulli(torch.full(size=labels.shape, fill_value=self.policy[1]/(1-self.policy[0]))).bool()\n",
        "        randomized_indices = randomized_indices & masked_indices & ~replaced_indices\n",
        "        random_tokens = torch.randint(high=len(self.tokenizer), size=labels.shape, dtype=torch.long)\n",
        "\n",
        "        allowed_token_ids = [i for i in range(len(self.tokenizer)) if i not in self.special_token_ids]\n",
        "        for special_token_id in self.special_token_ids:\n",
        "            replacement_token_id = np.random.choice(allowed_token_ids, size=1)\n",
        "            random_tokens[random_tokens == special_token_id] = torch.tensor(replacement_token_id, dtype=torch.long)\n",
        "\n",
        "        input_ids[randomized_indices] = random_tokens[randomized_indices]\n",
        "\n",
        "        return input_ids, labels\n"
      ],
      "metadata": {
        "id": "7CrGxafrLIE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                          do_lower_case=True,\n",
        "                                          do_basic_tokenize=True,\n",
        "                                          unk_token='[UNK]',\n",
        "                                          sep_token='[SEP]',\n",
        "                                          pad_token='[PAD]',\n",
        "                                          cls_token='[CLS]',\n",
        "                                          mask_token='[MASK]')\n",
        "\n",
        "encode = partial(TOKENIZER.encode, max_length=48)\n",
        "pad_token_id = TOKENIZER.convert_tokens_to_ids(TOKENIZER.pad_token)\n",
        "# NOTE: We add the pad token ID as discussed in https://github.com/pytorch/text/issues/609\n",
        "TEXT = Field(use_vocab=False, tokenize=encode, pad_token=pad_token_id, batch_first=True)\n",
        "LABEL = LabelField()"
      ],
      "metadata": {
        "id": "bprJA2Cn6OBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = IMDB.splits(text_field=TEXT, label_field=LABEL)\n",
        "LABEL.build_vocab(train_dataset)\n",
        "train_loader, test_loader = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyHWHeTq9ODK",
        "outputId": "caafce2d-5134-40f2-b08d-444145b93a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_label(label):\n",
        "    tokens = label.split()\n",
        "    tokens = [token for token in tokens if token != '[UNK]']\n",
        "    tokens = ['%d. %s' % (i + 1, token) for i, token in enumerate(tokens)]\n",
        "    return ', '.join(tokens)"
      ],
      "metadata": {
        "id": "CMm5E5NQ9QO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLM"
      ],
      "metadata": {
        "id": "OCKx8uq0-Zyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masking_schedule = [\n",
        "    (10000, [0.6, 0.3, 0.1])\n",
        "]\n",
        "\n",
        "collator = MLMCollator(tokenizer=TOKENIZER, mask_probability=0.25, masking_schedule=masking_schedule)\n",
        "# collator = MLMCollator(tokenizer=TOKENIZER, mask_probability=0.25)\n",
        "\n",
        "for batch in train_loader:\n",
        "    x, _ = batch.text, batch.label\n",
        "    examples, labels = collator(x)\n",
        "    print('Input:\\n------\\n%s \\n\\nOutput:\\n-------\\n%s' %\n",
        "          (TOKENIZER.decode(examples[0].tolist()), simplify_label(TOKENIZER.decode(labels[0].tolist()))))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOAJV-rv-ZSY",
        "outputId": "5cfae633-3eee-45c2-a4e0-291e62f36c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "------\n",
            "[CLS] [MASK] was prc thirteen [MASK] wrexham movie [MASK] [MASK] bosch television. it is far superior in action than most movies since. martin sheen is excellent, and though nick nolte has a small part, he too provides excellent support [MASK] vic [SEP] \n",
            "\n",
            "Output:\n",
            "-------\n",
            "1. i, 2. about, 3. when, 4. this, 5. came, 6. out, 7. on, 8. excellent, 9. [UNK].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "036u_twgA42n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}